---
layout: site
title: Sebastian Riedel
---

My name is Sebastian Riedel, and I'm currently a postdoctoral researcher at the "University of Massachusetts":http://www.umass.edu, working with "Andrew McCallum":http://www.cs.umass.edu/~mccallum/ in the "IESL lab":http://iesl.cs.umass.edu/.

In order to improve the world, we need to understand the processes that shape it. Whenever scientists advance our understanding of such processes, they write down their findings in publications, patents, webpages or the like. However, the sheer amount of text produced, and its ever-increasing nature, makes it impossible for a single entity to effectively grasp this knowledge. For example, there are about two thousand abstracts added daily to the over 18 million abstracts on biomedical research in the PubMed online repository. The overarching goal of my research is to extract and distill this collective but hidden and shattered knowledge from natural language text. 

To achieve this goal I believe progress needs to be made in several areas, and I contributed to advance each of these:

* Natural Language Processing components such as parsers or semantic role labellers ("riedel06incremental":http://riedelcastro.github.com/publications/details/riedel06incremental.html, "meza09jointly":http://riedelcastro.github.com/publications/details/meza09jointly.html).
* Efficient Inference in probabilistic models ("riedel08improving":http://riedelcastro.github.com/publications/details/riedel08improving.html, "riedel10relaxed":http://riedelcastro.github.com/publications/details/riedel10relaxed.html).
* Weak/distant supervision techniques to learn from existing structured knowledge sources ("riedel10modeling":http://riedelcastro.github.com/publications/details/riedel10modeling.html, "yao10collective":http://riedelcastro.github.com/publications/details/yao10collective.html).
* Joint Inference between traditionally pipelined components ("riedel11fast":http://riedelcastro.github.com/publications/details/riedel11fast.html, "meza09jointly":http://riedelcastro.github.com/publications/details/meza09jointly.html, "yoshikawa09jointly":http://riedelcastro.github.com/publications/details/yoshikawa09jointly.html).

Some of the above research has directly enabled me to develop state-of-the-art (and competition-winning) information extraction systems for biomedical literature: in 2005 my protein-protein interaction extraction model ranked 1st in the Learning Language in Logic Shared Task ("riedel05genic":http://riedelcastro.github.com/publications/details/riedel05genic.html); in 2009 I developed an event extraction system that ranked first in the BioNLP 2009 Shared Task, Track 2; in 2011, and in collaboration with colleagues in Stanford, I developed an event extractor for biomedical text that ranked 1st in three tracks of the BioNLP 2011 Shared Task ("riedel11robust":http://riedelcastro.github.com/publications/details/riedel11robust.html, "riedel11model":http://riedelcastro.github.com/publications/details/riedel11model.html).

Along with my interest in NLP and Machine Learning, I am also a strong believer of Probabilistic Programming/Statistical Relational Learning. I think that a major factor that slows down progress in AI is the difficulty of engineering large scale end-to-end systems that reason under uncertainty. Probabilistic Programming frameworks can dramatically simplify this process. To do my part in this I have developed "markov thebeast":http://code.google.com/p/thebeast/, a Markov Logic engine that is currently used in prestigious research institutes around the world and helps developers to create state-of-the-art probabilistic models without the need of a post-graduate degree in Machine Learning. 

See my "publications":/publications/all.html and "software":/software.html for more details and an exhaustive list of publications.

I am also interested in helping out the "Ishinomaki Children's Refuge Center":http://ishinomakicrc.wordpress.com/. 

